{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "mit_test = pd.read_csv(\"mitbih_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate x, y\n",
    "x_train_df = mit_train.loc[:, 0: 186]\n",
    "y_train_df = mit_train.loc[:, 187]\n",
    "\n",
    "x_test_df = mit_test.loc[:, 0: 186]\n",
    "y_test_df = mit_test.loc[:, 187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np = x_train_df.values\n",
    "y_train_np = y_train_df.values\n",
    "x_test_np = x_test_df.values\n",
    "y_test_np = y_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整类别数量\n",
    "# 0类降采样\n",
    "len_train_0 = len(y_train_np[y_train_np==0])\n",
    "index_train_0 = int(0.1*len_train_0)\n",
    "\n",
    "len_test_0 = len(y_test_np[y_test_np==0])\n",
    "index_test_0 = int(0.1*len_test_0)\n",
    "# 重新取样\n",
    "x_train_np_re = np.copy(x_train_np[y_train_np==0])\n",
    "np.random.shuffle(x_train_np_re)\n",
    "x_train_slected = x_train_np_re[0: index_train_0, :]\n",
    "\n",
    "x_test_np_re = np.copy(x_test_np[y_test_np==0])\n",
    "np.random.shuffle(x_test_np_re)\n",
    "x_test_selected = x_test_np_re[0: index_test_0, :]\n",
    "\n",
    "# 数据重新组合\n",
    "X_train = np.concatenate([x_train_slected, x_train_np[y_train_np!=0]])\n",
    "Y_train = np.concatenate([np.zeros(index_train_0), y_train_np[y_train_np!=0]])\n",
    "\n",
    "X_test = np.concatenate([x_test_selected, x_test_np[y_test_np!=0]])\n",
    "Y_test = np.concatenate([np.zeros(index_test_0), y_test_np[y_test_np!=0]])\n",
    "# 数量向量\n",
    "num = np.array([index_train_0, len(y_train_np[y_train_np==1]), len(y_train_np[y_train_np==2]), len(y_train_np[y_train_np==3]), len(y_train_np[y_train_np==4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, Y_Train, X_Test, Y_Test = map(\n",
    "    torch.from_numpy,\n",
    "    (X_train, Y_train, X_test, Y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train = X_Train.unsqueeze(1)\n",
    "X_Test = X_Test.unsqueeze(1)\n",
    "# Data Set\n",
    "bs = 128\n",
    "train_ds = TensorDataset(X_Train, Y_Train)\n",
    "test_ds = TensorDataset(X_Test, Y_Test)\n",
    "# Data Loader\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint_adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def norm(dim):\n",
    "    return nn.GroupNorm(min(32, dim), dim) # input_size = output_size Normalize\n",
    "\n",
    "class ConcatConv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, kernel_size=3, stride=1, padding=0, bias=True):\n",
    "        super(ConcatConv1d, self).__init__()\n",
    "        module = nn.Conv1d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        tt = torch.ones_like(x[:, :1, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n",
    "\n",
    "class ODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ODEfunc, self).__init__()\n",
    "        self.norm1 = norm(dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.conv2 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
    "        self.norm3 = norm(dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.norm3(out)\n",
    "        return out\n",
    "    \n",
    "class ODEnet(nn.Module): # Solver\n",
    "\n",
    "    def __init__(self, odefunc, rtol=1e-3, atol=1e-3):\n",
    "        super(ODEnet, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.forward_time = torch.tensor([0, 1]).float()\n",
    "        self.rtol = rtol\n",
    "        self.atol = atol\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.forward_time = self.forward_time.type_as(x)\n",
    "        out = odeint_adjoint(self.odefunc, x, self.forward_time, rtol=self.rtol, atol=self.atol)\n",
    "        return out[1]\n",
    "    \n",
    "    @property\n",
    "    def nfe(self):\n",
    "        return self.odefunc.nfe\n",
    "    \n",
    "    @nfe.setter\n",
    "    def nfe(self, value):\n",
    "        self.odefunc.nfe = value\n",
    "\n",
    "class Flatten(nn.Module): #  3D->2D linear input\n",
    "    def _init_(self):\n",
    "        super(Flatten, self)._init_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(dim=64, **kwargs):\n",
    "    # ODE net building\n",
    "    downsampling_layers = [\n",
    "        nn.Conv1d(1, dim, 3, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1),\n",
    "        norm(dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv1d(dim, dim, 4, 2, 1)\n",
    "    ]\n",
    "    # feature extraction layers\n",
    "    feature_layers = [ODEnet(ODEfunc(dim), **kwargs)]\n",
    "    # fc_layers\n",
    "    fc_layers = [\n",
    "        norm(dim), \n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.AdaptiveAvgPool1d(1),\n",
    "        Flatten(),\n",
    "        nn.Linear(dim, 5)\n",
    "        ]\n",
    "    \n",
    "    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers)\n",
    "    opt = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    return model, opt\n",
    "\n",
    "# loss function\n",
    "def criterion_bulid(weight_vector,weight=False):\n",
    "    if weight:\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(weight_vector).float())\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    return criterion\n",
    "\n",
    "def fit(epochs, model, criterion, opt, train_dl, valid_dl):\n",
    "    num_batches = len(train_dl)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch{epoch + 1}\")\n",
    "        model.train() #training mode\n",
    "        for xt, yt in train_dl:\n",
    "            batch_count+=1\n",
    "            xt = xt.to(device)\n",
    "            yt = yt.to(device)\n",
    "\n",
    "            output_train = model(xt.float())\n",
    "            loss = criterion(output_train, yt.long())\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        model.eval() # validation mode\n",
    "        predicted_labels = []\n",
    "        actual_labels = []\n",
    "        for xv, yv in valid_dl:\n",
    "            xv = xv.to(device)\n",
    "            yv=  yv.to(device)\n",
    "            output_valid = model(xv.float())\n",
    "            _, predicted = torch.max(output_valid, 1)\n",
    "            predicted_labels.extend(predicted.tolist())\n",
    "            actual_labels.extend(yv.tolist())\n",
    "        \n",
    "        correct_prediction = sum(p == a for p, a in zip(predicted_labels, actual_labels))\n",
    "        accuracy = correct_prediction / len(actual_labels)\n",
    "\n",
    "        print(f'validation accuracy: {accuracy * 100:.2f}%')\n",
    "        return predicted_labels, actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = criterion_bulid(1/num, weight=True)\n",
    "odenet, odeopt = model_building(dim=64, rtol=1e-3, atol=1e-3)\n",
    "fit(epochs=5, model=odenet, criterion=criterion, opt=odeopt, train_dl=train_dl, valid_dl=test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
